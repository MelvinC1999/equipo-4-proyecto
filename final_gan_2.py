# -*- coding: utf-8 -*-
"""Final GAN 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qbYu02fmMf0RUOATYdvnARsOqKsX9vHa
"""

#Celda 1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, KFold
import tensorflow as tf
from tensorflow.keras.models import Model, load_model, Sequential
from tensorflow.keras.layers import Input, Dense, LeakyReLU
from tensorflow.keras.optimizers import Adam
import os
import pickle

# Cargar el archivo CSV
file_path = 'DatosCompletos2014 - modificado.csv'
data = pd.read_csv(file_path, delimiter=';')

# Convertir la columna "Horas" a segundos
def time_to_seconds(time_str):
    h, m, s = map(int, time_str.split('.'))
    return h * 3600 + m * 60 + s

data['Horas'] = data['Horas'].apply(time_to_seconds)

# Convertir la columna 'Horas' a categorías de tiempo del día
def categorize_time(seconds):
    hour = seconds // 3600
    if 6 <= hour < 12:
        return 'Mañana'
    elif 12 <= hour < 18:
        return 'Tarde'
    else:
        return 'Noche'

data['Tiempo_Dia'] = data['Horas'].apply(categorize_time)

# Convertir 'Tiempo_Dia' a numérico utilizando codificación de etiquetas
data['Tiempo_Dia'] = data['Tiempo_Dia'].astype('category').cat.codes

# Eliminar columnas categóricas no necesarias
data = data.drop(columns=['Tipo_Documento', 'Sexo', 'Estad_Civil', 'Titulo_Prof', 'Ocupacion'])

# Convertir la columna 'Transaccion' a numérica utilizando codificación de etiquetas
data['Transaccion'] = data['Transaccion'].astype('category').cat.codes

# Eliminar filas con valores nulos
data = data.dropna()

# Información básica sobre los datos después de la conversión
print(data.info())
print(data.head())

# Guardar los valores originales antes de la normalización
original_values = data.copy()

# Preparar los datos para el autoencoder
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data[['Valor', 'Tiempo_Dia', 'Dia_Semana', 'Mes', 'Dia_Mes', 'Transaccion']])

# Dividir los datos en entrenamiento y prueba con validación cruzada
kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold = 1
for train_index, val_index in kf.split(scaled_data):
    X_train, X_val = scaled_data[train_index], scaled_data[val_index]

    # Construir el autoencoder
    input_dim = X_train.shape[1]
    encoding_dim = 2
    input_layer = Input(shape=(input_dim,))
    encoder = Dense(encoding_dim, activation="tanh")(input_layer)
    decoder = Dense(input_dim, activation="tanh")(encoder)
    autoencoder = Model(inputs=input_layer, outputs=decoder)
    autoencoder.compile(optimizer='adam', loss='mean_squared_error')

    # Entrenar el autoencoder
    history = autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_val, X_val), verbose=1)

    # Monitorización del entrenamiento
    plt.plot(history.history['loss'], label='Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.legend()
    plt.title(f'Fold {fold}')
    plt.show()

    fold += 1

# Detectar valores atípicos
reconstructions = autoencoder.predict(scaled_data)
reconstruction_errors = np.mean(np.abs(reconstructions - scaled_data), axis=1)
threshold = np.percentile(reconstruction_errors, 95)
outliers = reconstruction_errors > threshold

# Eliminar los valores atípicos detectados
data_cleaned = data[~outliers]
original_values_cleaned = original_values[~outliers]

# Mostrar la información del nuevo conjunto de datos sin outliers
print(data_cleaned.info())
print(data_cleaned.head())

#Celda 2
# Seleccionar los 5 clientes con más transacciones
top_clients = data_cleaned['Cod_Cliente'].value_counts().head(5).index.tolist()
print(f'Top 5 clientes con más transacciones: {top_clients}')

scalers = {}
for client_id in top_clients:
    client_data = data_cleaned[data_cleaned['Cod_Cliente'] == client_id]
    scaler = MinMaxScaler(feature_range=(-1, 1))
    client_data_scaled = scaler.fit_transform(client_data[['Valor', 'Tiempo_Dia', 'Dia_Semana', 'Mes', 'Dia_Mes', 'Transaccion']])
    scalers[client_id] = scaler
    data_cleaned.loc[data_cleaned['Cod_Cliente'] == client_id, ['Valor', 'Tiempo_Dia', 'Dia_Semana', 'Mes', 'Dia_Mes', 'Transaccion']] = client_data_scaled

# Guardar los scalers en archivos
scaler_dir = 'scalers'
os.makedirs(scaler_dir, exist_ok=True)
for client_id, scaler in scalers.items():
    with open(os.path.join(scaler_dir, f'scaler_{client_id}.pkl'), 'wb') as f:
        pickle.dump(scaler, f)

print(data_cleaned.info())
print(data_cleaned.head())

# Guardar los valores originales en un archivo
original_values_cleaned.to_csv('original_values_cleaned.csv', index=False)

#Celda 3
# Función para entrenar los modelos GAN por cliente con validación cruzada
def train_gan_per_client(client_data, selected_clients, epochs=50, batch_size=64):
    results = {}
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    for client_id in selected_clients:
        group = client_data[client_data['Cod_Cliente'] == client_id].values
        input_dim = 100
        X = group[:, 1:]  # Excluir 'Cod_Cliente'

        for train_index, val_index in kf.split(X):
            X_train, X_val = X[train_index], X[val_index]

            # Crear modelos
            generator = Sequential()
            generator.add(Dense(128, input_dim=input_dim))
            generator.add(LeakyReLU(alpha=0.01))
            generator.add(Dense(256))
            generator.add(LeakyReLU(alpha=0.01))
            generator.add(Dense(512))
            generator.add(LeakyReLU(alpha=0.01))
            generator.add(Dense(X_train.shape[1], activation='tanh'))

            discriminator = Sequential()
            discriminator.add(Dense(512, input_dim=X_train.shape[1]))
            discriminator.add(LeakyReLU(alpha=0.01))
            discriminator.add(Dense(256))
            discriminator.add(LeakyReLU(alpha=0.01))
            discriminator.add(Dense(128))
            discriminator.add(LeakyReLU(alpha=0.01))
            discriminator.add(Dense(1, activation='sigmoid'))
            discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])

            gan = Sequential()
            gan.add(generator)
            gan.add(discriminator)
            discriminator.trainable = False
            gan.compile(loss='binary_crossentropy', optimizer=Adam())

            for epoch in range(epochs):
                idx = np.random.randint(0, X_train.shape[0], batch_size)
                real_data = X_train[idx]

                noise = np.random.normal(0, 1, (batch_size, input_dim))
                generated_data = generator.predict(noise)

                real_labels = np.ones((batch_size, 1))
                fake_labels = np.zeros((batch_size, 1))

                d_loss_real = discriminator.train_on_batch(real_data, real_labels)
                d_loss_fake = discriminator.train_on_batch(generated_data, fake_labels)
                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

                noise = np.random.normal(0, 1, (batch_size, input_dim))
                g_loss = gan.train_on_batch(noise, real_labels)

                if epoch % 10 == 0 or epoch == epochs - 1:
                    print(f"Cliente {client_id}, Época {epoch} [D loss: {d_loss[0]} | D accuracy: {d_loss[1] * 100}] [G loss: {g_loss}]")

            generator.save(f'gan_models/generator_{client_id}.h5')
            discriminator.save(f'gan_models/discriminator_{client_id}.h5')

        results[client_id] = (generator, discriminator, scalers[client_id])

    return results

# Crear el directorio para guardar los modelos si no existe
os.makedirs('gan_models', exist_ok=True)

# Entrenar y guardar los modelos GAN para los clientes seleccionados
gan_results = train_gan_per_client(data_cleaned, top_clients)

#Celda 4
import os
import numpy as np
import pandas as pd
from keras.models import load_model
from sklearn.preprocessing import MinMaxScaler
import pickle

# Función para convertir el tiempo en segundos a un formato legible
def seconds_to_time(seconds):
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    seconds = seconds % 60
    return f"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}"

# Cargar los modelos y scalers
def load_models_and_scalers(model_dir, scaler_dir, selected_clients):
    models = {}
    for client_id in selected_clients:
        generator_path = os.path.join(model_dir, f'generator_{client_id}.h5')
        discriminator_path = os.path.join(model_dir, f'discriminator_{client_id}.h5')

        if os.path.exists(generator_path) and os.path.exists(discriminator_path):
            generator = load_model(generator_path, compile=False)
            discriminator = load_model(discriminator_path, compile=False)

            scaler_path = os.path.join(scaler_dir, f'scaler_{client_id}.pkl')
            if os.path.exists(scaler_path):
                with open(scaler_path, 'rb') as f:
                    scaler = pickle.load(f)
            else:
                print(f"Scaler para el cliente {client_id} no encontrado.")
                scaler = None

            models[client_id] = (generator, discriminator, scaler)
        else:
            print(f"Modelos para el cliente {client_id} no encontrados.")
            models[client_id] = (None, None, None)

    return models

# Asegúrate de tener los modelos y los scalers para los clientes seleccionados
gan_models_dir = 'gan_models'
scaler_dir = 'scalers'
gan_results = load_models_and_scalers(gan_models_dir, scaler_dir, top_clients)

# Definir las transacciones normales y anómalas
normal_transactions = [
    {"Horas": "09.57.43", "Dia_Semana": 3, "Mes": 10, "Dia_Mes": 14, "Cod_Cliente": 1599382, "Transaccion": "TRANSFER INTERNAS", "Valor": 993.8},
    {"Horas": "11.00.43", "Dia_Semana": 4, "Mes": 7, "Dia_Mes": 30, "Cod_Cliente": 1599382, "Transaccion": "TRANSFER BANCOS", "Valor": 97.48},
    {"Horas": "11.41.11", "Dia_Semana": 3, "Mes": 12, "Dia_Mes": 30, "Cod_Cliente": 1599382, "Transaccion": "TRANSFER BANCOS", "Valor": 96.68},
    {"Horas": "16.50.23", "Dia_Semana": 3, "Mes": 9, "Dia_Mes": 16, "Cod_Cliente": 1599382, "Transaccion": "TRANSFER BANCOS", "Valor": 92.17},
    {"Horas": "18.06.55", "Dia_Semana": 4, "Mes": 8, "Dia_Mes": 6, "Cod_Cliente": 1599382, "Transaccion": "TRANSFER BANCOS", "Valor": 91.89},
    {"Horas": "15.54.12", "Dia_Semana": 4, "Mes": 8, "Dia_Mes": 27, "Cod_Cliente": 1599382, "Transaccion": "TRANSFER BANCOS", "Valor": 91.89},
    {"Horas": "12.55.47", "Dia_Semana": 2, "Mes": 9, "Dia_Mes": 8, "Cod_Cliente": 1599382, "Transaccion": "TRANSFER BANCOS", "Valor": 91.89},
    {"Horas": "12.55.47", "Dia_Semana": 2, "Mes": 9, "Dia_Mes": 8, "Cod_Cliente": 1599382, "Transaccion": "TRANSFER BANCOS", "Valor": 10000.00},
    {"Horas": "09.32.08", "Dia_Semana": 2, "Mes": 9, "Dia_Mes": 1, "Cod_Cliente": 1598973, "Transaccion": "TRANSFER BANCOS", "Valor": 3000.00}
]

anomalous_transactions = [
    {"Horas": "09.32.08", "Dia_Semana": 2, "Mes": 9, "Dia_Mes": 1, "Cod_Cliente": 1599382, "Transaccion": "TRANSFER BANCOS", "Valor": 20000.00},
    {"Horas": "09.32.08", "Dia_Semana": 2, "Mes": 9, "Dia_Mes": 1, "Cod_Cliente": 1598973, "Transaccion": "TRANSFER BANCOS", "Valor": 5000.00},
    {"Horas": "09.32.08", "Dia_Semana": 2, "Mes": 9, "Dia_Mes": 1, "Cod_Cliente": 1598973, "Transaccion": "TRANSFER BANCOS", "Valor": 10000.00}
]

# Función para predecir y explicar si una transacción es anómala
def predict_anomaly_per_client(transaction, gan_results, data_cleaned, original_values_cleaned):
    client_id = transaction['Cod_Cliente']

    if client_id not in gan_results or gan_results[client_id][0] is None:
        print(f"No se encontraron los modelos para el cliente {client_id}.")
        return None

    generator, discriminator, scaler = gan_results[client_id]

    if scaler is None:
        print(f"No se encontró el scaler para el cliente {client_id}.")
        return None

    transaction_df = pd.DataFrame([transaction])
    transaction_df['Horas'] = transaction_df['Horas'].apply(time_to_seconds)

    # Codificar las columnas categóricas como numéricas
    transaction_df['Transaccion'] = transaction_df['Transaccion'].astype('category').cat.codes

    # Agregar la columna Tiempo_Dia utilizando la misma lógica que en la celda 1
    transaction_df['Tiempo_Dia'] = transaction_df['Horas'].apply(categorize_time).astype('category').cat.codes

    # Guardar valores originales para la explicación
    original_values = transaction_df.copy()

    # Normalizar los valores de la transacción
    features = ['Valor', 'Tiempo_Dia', 'Dia_Semana', 'Mes', 'Dia_Mes', 'Transaccion']
    transaction_df[features] = scaler.transform(transaction_df[features])

    transaction_scaled = transaction_df[features].values

    noise = np.random.normal(0, 1, (1, 100))
    generated_data = generator.predict(noise)

    # Obtener los datos históricos del cliente
    client_data = data_cleaned[data_cleaned['Cod_Cliente'] == client_id][features].values

    if client_data.size == 0:
        print(f"No hay datos históricos para el cliente {client_id}.")
        return None

    # Calcula la distancia solo para la variable 'Valor'
    distance = np.abs(transaction_scaled[0, 0] - generated_data[0, 0])

    distances = np.abs(client_data[:, 0] - generated_data[:, 0])

    threshold = np.percentile(distances, 95)
    is_anomalous = distance > threshold

    # Obtener los valores históricos originales
    original_client_data = original_values_cleaned[original_values_cleaned['Cod_Cliente'] == client_id]

    max_historical_value = original_client_data['Valor'].max()
    most_frequent_hour = seconds_to_time(original_client_data['Horas'].mode()[0])
    most_frequent_day = original_client_data['Dia_Semana'].mode()[0]
    most_frequent_transaction = original_client_data['Transaccion'].mode()[0]
    most_frequent_value = original_client_data['Valor'].mode()[0]

    contributing_variable = 'Valor'
    variable_difference = distance

    # Desnormalizar el valor para la explicación
    real_value = scaler.inverse_transform(transaction_scaled)[0, 0]
    real_threshold = scaler.inverse_transform([[threshold] + [0] * (transaction_scaled.shape[1] - 1)])[0, 0]
    real_distance = np.abs(original_values['Valor'].values[0] - real_value)

    result = {
        'is_anomalous': bool(is_anomalous),
        'distance': float(real_distance),
        'threshold': float(real_threshold),
        'contributing_variable': contributing_variable,
        'variable_difference': variable_difference,
        'max_historical_value': max_historical_value,
        'most_frequent_hour': most_frequent_hour,
        'most_frequent_day': most_frequent_day,
        'most_frequent_transaction': most_frequent_transaction,
        'most_frequent_value': most_frequent_value,
        'real_value': real_value
    }

    return result

# Probar las transacciones normales y anómalas
for transaction in normal_transactions + anomalous_transactions:
    result = predict_anomaly_per_client(transaction, gan_results, data_cleaned, original_values_cleaned)
    if result is None:
        continue

    if result['is_anomalous']:
        message = (
            f"La transacción con {transaction['Transaccion']} de valor {transaction['Valor']} realizada el {transaction['Dia_Mes']}/{transaction['Mes']} a las {transaction['Horas']} "
            f"ha sido clasificada como anómala. La principal razón es que la variable {result['contributing_variable']} presenta una diferencia significativa de "
            f"{result['variable_difference']} en comparación con los datos históricos del cliente {transaction['Cod_Cliente']}. "
            f"La distancia calculada es {result['distance']}, que excede el umbral de {result['threshold']}. "
            f"El valor máximo histórico para este cliente es {result['max_historical_value']}. Los horarios más frecuentes son alrededor de las {result['most_frequent_hour']} y los días más frecuentes son los días de la semana {result['most_frequent_day']}. "
            f"El tipo de transacción más frecuente en montos altos es {result['most_frequent_transaction']}. El valor más frecuente es {result['most_frequent_value']}."
        )
    else:
        message = (
            f"La transacción con {transaction['Transaccion']} de valor {transaction['Valor']} realizada el {transaction['Dia_Mes']}/{transaction['Mes']} a las {transaction['Horas']} "
            f"ha sido clasificada como normal. La distancia calculada es {result['distance']}, que está por debajo del umbral de {result['threshold']}. "
            f"El valor máximo histórico para este cliente es {result['max_historical_value']}. Los horarios más frecuentes son alrededor de las {result['most_frequent_hour']} y los días más frecuentes son los días de la semana {result['most_frequent_day']}. "
            f"El tipo de transacción más frecuente en montos altos es {result['most_frequent_transaction']}. El valor más frecuente es {result['most_frequent_value']}."
        )
    print(message)

# Métricas de desempeño del modelo
true_positives = sum(1 for transaction in anomalous_transactions if predict_anomaly_per_client(transaction, gan_results, data_cleaned, original_values_cleaned) is not None and predict_anomaly_per_client(transaction, gan_results, data_cleaned, original_values_cleaned)['is_anomalous'])
false_negatives = sum(1 for transaction in anomalous_transactions if predict_anomaly_per_client(transaction, gan_results, data_cleaned, original_values_cleaned) is not None and not predict_anomaly_per_client(transaction, gan_results, data_cleaned, original_values_cleaned)['is_anomalous'])
true_negatives = sum(1 for transaction in normal_transactions if predict_anomaly_per_client(transaction, gan_results, data_cleaned, original_values_cleaned) is not None and not predict_anomaly_per_client(transaction, gan_results, data_cleaned, original_values_cleaned)['is_anomalous'])
false_positives = sum(1 for transaction in normal_transactions if predict_anomaly_per_client(transaction, gan_results, data_cleaned, original_values_cleaned) is not None and predict_anomaly_per_client(transaction, gan_results, data_cleaned, original_values_cleaned)['is_anomalous'])

# Verificar que no haya división por cero
total_predictions = true_positives + true_negatives + false_positives + false_negatives
if total_predictions == 0:
    print("No se pueden calcular las métricas de desempeño ya que no hay suficientes predicciones.")
else:
    accuracy = (true_positives + true_negatives) / total_predictions
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    print(f"Precisión (Accuracy): {accuracy:.4f}")
    print(f"Precisión (Precision): {precision:.4f}")
    print(f"Exhaustividad (Recall): {recall:.4f}")
    print(f"F1 Score: {f1_score:.4f}")
    print(f"True Positives: {true_positives}")
    print(f"True Negatives: {true_negatives}")
    print(f"False Positives: {false_positives}")
    print(f"False Negatives: {false_negatives}")

